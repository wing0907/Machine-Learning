from sklearn.datasets import load_breast_cancer
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier, XGBRegressor
import random
import numpy as np
from sklearn.model_selection import train_test_split
import pandas as pd
import xgboost as xgb
# print(xgb.__version__)
from sklearn.metrics import accuracy_score, r2_score
import sklearn as sk
# print(sk.__version__) # 1.6.1
import warnings
warnings.filterwarnings('ignore')


seed = 123
random.seed(seed)
np.random.seed(seed)

# 1. ë°ì´í„°
datasets = load_breast_cancer()
x = datasets.data
y = datasets.target
print(x.shape, y.shape)  

x_train, x_test, y_train, y_test = train_test_split(
    x, y, train_size=0.8, random_state=seed,
    stratify=y
)



es = xgb.callback.EarlyStopping(
    rounds = 50,
    metric_name = 'logloss',
    data_name = 'validation_0',
    # save_best = True,
    
)

# 2. ëª¨ë¸êµ¬ì„±
model = XGBClassifier(
    n_estimators = 500,
    max_depth = 6,
    gamma = 0,
    min_child_weight = 0,
    subsample = 0.4,
    reg_alpha = 0,
    reg_lambda = 1,                 # íšŒê·€ : rmse, mae, rmsle
    eval_metric = 'logloss',     # ë‹¤ì¤‘ë¶„ë¥˜ : mlogloss, merror // ì´ì§„ë¶„ë¥˜ : logloss, error
                                # 2.1.1ë²„ì „ ì´í›„ë¡œ fitì—ì„œ ëª¨ë¸ë¡œ ìœ„ì¹˜ì´ë™.
    callbacks = [es],
    random_state=seed
    
    
    )


model.fit(x_train, y_train,
          eval_set = [(x_test, y_test)],
          verbose=0,
          )


print('acc : ', model.score(x_test, y_test))
# print(model.feature_importances_)
# [0.05029093 0.07584832 0.52197117 0.35188958]             ì–˜ë„¤ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì œê±°í–ˆì„ ë•Œ, ì„±ëŠ¥ ë‚˜ì˜¤ê² ì£ ?
                                                            # ë‘ê°œë¥¼ ì œê±°í–ˆì„ ë•Œ, ì„¸ê°œë¥¼ ì œê±°í–ˆì„ ë•Œ, ì„±ëŠ¥ ë‚˜ì˜¤ê² ì£ ?
                                                            # í•œë²ˆì— ë³´ê²Œ í• ê±°ì•¼. ì•ì—ì„œëŠ” 25%ë¥¼ í‰ ì³¤ì£ ? ê·¸ëŸ¬ë©´ ì¤‘ê°„ì—
                                                            # í–¥ìƒ ë˜ëŠ” ê±¸ ëª» ë³´ì–ì•„~~ ê·¸ì£ ì˜¹?? ì•„ ê·¸ë ‡ì–ì•„~~
                                                            # ê·¸ë˜ì„œ ê·¸ êµ¬ê°„ì„ ì°¾ì•„ë‚´ëŠ”ê²Œ ìš°ë¦¬ì˜ ëª©ì ì´ë‹¤ì‰?!?!
# aaa = model.get_booster().get_score(importance_type='weight') # split ë¹ˆë„ìˆ˜ ê°œë…
# {'f0': 121.0, 'f1': 66.0, 'f2': 11.0, 'f3': 3.0, 'f4': 28.0, 'f5': 16.0, 
#  'f6': 25.0, 'f7': 92.0, 'f8': 35.0, 'f9': 18.0, 'f10': 21.0, 'f11': 11.0, 
#  'f12': 7.0, 'f13': 32.0, 'f14': 21.0, 'f15': 13.0, 'f16': 22.0, 'f17': 8.0, 
#  'f18': 23.0, 'f19': 18.0, 'f20': 63.0, 'f21': 35.0, 'f22': 40.0, 'f23': 6.0, 
#  'f24': 18.0, 'f25': 7.0, 'f26': 20.0, 'f27': 60.0, 'f28': 22.0, 'f29': 3.0}

from sklearn.feature_selection import SelectFromModel

aaa = model.get_booster().get_score(importance_type='gain')
# {'f0': 0.09054344147443771, 'f1': 0.44619157910346985, 'f2': 0.9568471312522888, 
#  'f3': 0.22348596155643463, 'f4': 0.20814745128154755, 'f5': 0.39905595779418945, 
#  'f6': 0.027111342176795006, 'f7': 0.824626088142395, 'f8': 0.19422045350074768, 
#  'f9': 0.15491503477096558, 'f10': 0.40824103355407715, 'f11': 0.35078516602516174, 
#  'f12': 0.27346092462539673, 'f13': 0.539326548576355, 'f14': 0.14593474566936493, 
#  'f15': 0.4656308889389038, 'f16': 0.4021739661693573, 'f17': 0.1481102705001831, 
#  'f18': 0.19117841124534607, 'f19': 0.22889749705791473, 'f20': 0.5641250610351562, 
#  'f21': 0.8972993493080139, 'f22': 5.388490676879883, 'f23': 0.6300467848777771, 
#  'f24': 0.8262636661529541, 'f25': 0.1927350014448166, 'f26': 1.107054352760315, 
#  'f27': 0.9051604866981506, 'f28': 0.22356440126895905, 'f29': 0.06334438920021057}
print("Original aaa (dict):", aaa)

gain_list = list(aaa.values())      # ë”•ì…”ë„ˆë¦¬ ê°’ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
print("\ngain_list (values from dict):", gain_list)

gain_list = [aaa.get(f'f{i}', 0) for i in range(x_train.shape[1])]
gain_array = np.array(gain_list)
normalized_gain = (gain_array - gain_array.min()) / (gain_array.max() - gain_array.min())


# # --- Min-Max ì •ê·œí™” ì ìš© ---
# min_gain = np.min(gain_list)
# max_gain = np.max(gain_list)

# # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ max_gainê³¼ min_gainì´ ê°™ì€ ê²½ìš° ì²˜ë¦¬
# if max_gain == min_gain:
#     normalized_gain_list = np.zeros_like(gain_list, dtype=float)
# else:
#     normalized_gain_list = (gain_list - min_gain) / (max_gain - min_gain)

# print("\nnormalized_gain_list (Min-Max ì •ê·œí™”):", normalized_gain_list)

# ì •ê·œí™”ëœ ê°’ì„ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ thresholdsë¡œ ì‚¬ìš©
thresholds = np.sort(normalized_gain)
print("\nthresholds (normalized and sorted):", thresholds)


feature_names = [f'f{i}' for i in range(len(gain_list))]
df_gain = pd.DataFrame({
    'Feature': feature_names,
    'Gain': gain_list,
    'Normalized_Gain': normalized_gain
})


# 3. ì •ë ¬
df_gain = df_gain.sort_values(by='Normalized_Gain')

# 4. ê¸°ì¤€ ì ìš© (ì˜ˆ: Normalized_Gain < 0.1)
drop_candidates = df_gain[df_gain['Normalized_Gain'] < 0.1]



### ì •ê·œí™”ëœ `thresholds`ë¥¼ ì‚¬ìš©í•œ Feature Selection ë° ëª¨ë¸ í›ˆë ¨

for i in thresholds:
    selection = SelectFromModel(model, threshold=i, prefit=True) 
    mask = selection.get_support()
    reverse_mask = [not i for i in mask]
    
    select_x_train = selection.transform(x_train)
    select_x_test = selection.transform(x_test)
    
    # Check if any features are selected to avoid errors
    if select_x_train.shape[1] == 0:
        print(f'Trech={i:.3f}: No features selected. Skipping model training.')
        continue

    select_model = XGBClassifier(
        n_estimators = 500,
        max_depth = 6,
        gamma = 0,
        min_child_weight = 0,
        subsample = 0.4,
        reg_alpha = 0,
        reg_lambda = 1,
        eval_metric = 'logloss', # XGBoost 2.1.1 ë²„ì „ ì´í›„ fitì—ì„œ ëª¨ë¸ë¡œ ìœ„ì¹˜ì´ë™.
        callbacks = [es],
        random_state=seed,
    ) 
    
    select_model.fit(select_x_train, y_train,
                     eval_set = [(select_x_test, y_test)],
                     verbose = 0)
    
    select_y_pred = select_model.predict(select_x_test)
    score = accuracy_score(y_test, select_y_pred)
    print("==================================================================")
    print(f'Trech={i:.3f}, n={select_x_train.shape[1]}, ACC: {score*100:.4f}%')
    print(datasets.feature_names[reverse_mask])

    
print("ğŸ§¹ ì œê±°í•´ë„ ë  ê°€ëŠ¥ì„±ì´ ë†’ì€ ì»¬ëŸ¼ë“¤:")
print(drop_candidates)
    
    
# Trech=0.002, n=30, ACC: 95.6140%
# Trech=0.004, n=29, ACC: 95.6140%
# Trech=0.005, n=28, ACC: 95.6140%
# Trech=0.008, n=27, ACC: 95.6140%
# Trech=0.008, n=26, ACC: 95.6140%
# Trech=0.009, n=25, ACC: 95.6140%
# Trech=0.011, n=24, ACC: 95.6140%
# Trech=0.011, n=23, ACC: 95.6140%
# Trech=0.011, n=22, ACC: 95.6140%
# Trech=0.012, n=21, ACC: 95.6140%
# Trech=0.013, n=20, ACC: 95.6140%
# Trech=0.013, n=19, ACC: 95.6140%
# Trech=0.013, n=18, ACC: 95.6140%
# Trech=0.016, n=17, ACC: 95.6140%
# Trech=0.020, n=16, ACC: 95.6140%
# Trech=0.023, n=15, ACC: 95.6140%
# Trech=0.023, n=14, ACC: 95.6140%
# Trech=0.023, n=13, ACC: 95.6140%
# Trech=0.026, n=12, ACC: 95.6140%
# Trech=0.027, n=11, ACC: 95.6140%
# Trech=0.031, n=10, ACC: 95.6140%
# Trech=0.032, n=9, ACC: 95.6140%
# Trech=0.036, n=8, ACC: 95.6140%
# Trech=0.047, n=7, ACC: 95.6140%
# Trech=0.047, n=6, ACC: 94.7368%
# Trech=0.051, n=5, ACC: 94.7368%
# Trech=0.052, n=4, ACC: 91.2281%
# Trech=0.055, n=3, ACC: 91.2281%
# Trech=0.063, n=2, ACC: 91.2281%
# Trech=0.308, n=1, ACC: 92.1053%







exit()
print("25%ì§€ì  : ", np.percentile(model.feature_importances_, 25)) # 25% ì§€ì ì„ ì¶œë ¥í•˜ê²Œì’ˆ~!
# 0.02461671084165573

percentile =  np.percentile(model.feature_importances_, 25)
print(type(percentile))     # <class 'numpy.float64'>

col_name = []
# ì‚­ì œí•  ì»¬ëŸ¼(25% ì´í•˜ì¸ ë†ˆ)ì„ ì°¾ì•„ë‚´ì!!!
for i, fi in enumerate(model.feature_importances_): # index ì™€ ê°’(fi)
    # print(i, fi)
    if fi <= percentile:        # ê°’ì´ ë‚®ì€ ë†ˆì„ ì°¾ì•„
        col_name.append(datasets.feature_names[i])  # col_nameì— ì§‘ì–´ë„£ì–´
    else:
        continue
print(col_name)         # ['sepal length (cm)']

x = pd.DataFrame(x, columns=datasets.feature_names) # ì–˜ëŠ” ìœ„ì—ì„œ ë§Œë“¤ì—ˆì–´ë„ ëœë‹¤
x = x.drop(columns=col_name)                        # ê³ ë†ˆì„ ì‚­ì œí•´.

print(x)


x_train, x_test, y_train, y_test = train_test_split(
    x, y, train_size=0.8, random_state=seed,
    stratify=y
)

model.fit(x_train, y_train)
print('acc :', model.score(x_test, y_test))         # acc : 0.9333333333333333


# tqdm ê°„ì§€ë‚˜ëŠ” progress bar

# import matplotlib.pyplot as plt

# def plot_feature_importance_datasets(model):
#     n_features = datasets.data.shape[1]
#     plt.barh(np.arange(n_features), model.feature_importances_, align='center')
#     # ìˆ˜í‰ ë§‰ëŒ€ ê·¸ë˜í”„, 4ê°œì˜ ì—´ì˜ feature importance ê·¸ë˜í”„, ê°’ ìœ„ì¹˜ ì„¼í„°
#     plt.yticks(np.arange(n_features), model.feature_importances_)
#     # ëˆˆê¸ˆ, ìˆ«ì ë ˆì´ë¸” í‘œì‹œ
#     plt.xlabel("feature Importance")
#     plt.ylabel("Feature")
#     plt.ylim(-1, n_features)        # ì¶• ë²”ìœ„ ì„¤ì •
#     plt.title(model.__class__.__name__)

# plot_feature_importance_datasets(model)
# plt.show()



